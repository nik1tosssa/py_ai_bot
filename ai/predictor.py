import os
import tensorflow as tf
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ü–£–¢–ï–ô ---
# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ 'models' –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(BASE_DIR, 'models')
TOKENIZERS_DIR = os.path.join(BASE_DIR, 'tokenizers')

MODEL_PATH = os.path.join(MODELS_DIR, 'complexity_model.keras')
TOKENIZER_PATH = os.path.join(TOKENIZERS_DIR, 'tokenizer.pickle')


class XPAnalyst:
    def __init__(self, model_path=MODEL_PATH, tokenizer_path=TOKENIZER_PATH):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –ø–∞–ø–∫–∏ models"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            self.model = tf.keras.models.load_model(model_path, compile=False)
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            with open(tokenizer_path, 'rb') as f:
                self.tokenizer = pickle.load(f)
            self.is_ready = True
            print("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤–∞!")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–∫—Ç–∏–≤–æ–≤: {e}")
            self.is_ready = False

    def analyze(self, text: str):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π XP"""
        if not self.is_ready:
            return None

        MAX_LEN = 20  # –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏

        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        sequence = self.tokenizer.texts_to_sequences([text])
        padded = pad_sequences(sequence, maxlen=MAX_LEN)

        # 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (—Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤—ã—Ö–æ–¥ ‚Äî —Å–ª–æ–∂–Ω–æ—Å—Ç—å)
        prediction = self.model.predict(padded, verbose=0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
        comp = float(prediction[0][0])

        # 3. –†–∞—Å—á–µ—Ç XP –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        # –ù–∞–ø—Ä–∏–º–µ—Ä: —Å–ª–æ–∂–Ω–æ—Å—Ç—å (1-10) * –±–∞–∑–æ–≤—É—é —Å—Ç–∞–≤–∫—É 100
        total_xp = int(max(0, comp * 100))

        return {
            "text": text,
            "complexity": round(comp, 2),
            "xp": total_xp,
            "status": self._get_simple_status(comp)
        }

    def _get_simple_status(self, comp):
        """–°—Ç–∞—Ç—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        if comp > 7: return "üèÜ –≠–ø–∏—á–Ω–æ"
        if comp > 4: return "‚ö°Ô∏è –ù–µ–ø—Ä–æ—Å—Ç–æ"
        return "üå± –õ–µ–≥–∫–æ"


# --- –¢–ï–°–¢ ---
if __name__ == "__main__":
    analyst = XPAnalyst()
    res = analyst.analyze("–°–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª —Å–ø–æ—Ä—Ç–∫–∞—Ä")
    if res:
        print(f"–¢–µ–∫—Å—Ç: {res['text']} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {res['complexity']} | XP: {res['xp']}")
import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ---
# –í—Å–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ 'models', –∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª
MODELS_DIR = 'models'
TOKENIZER_DIR = 'tokenizers'
DATASET_DIR = 'dataset'
DATASET_PATH = os.path.join(DATASET_DIR, 'dataset.csv')

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
MAX_WORDS = 10000
MAX_LEN = 30


def load_data(path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫ —Ç–∏–ø–∞ (Dtype error)."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å —É—á–µ—Ç–æ–º —Ç–≤–æ–µ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    try:
        df = pd.read_csv(path, sep=';', encoding='utf-16')
    except Exception:
        df = pd.read_csv(path, sep=';', encoding='utf-8')

    # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
    df.columns = df.columns.str.strip()

    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–®–ò–ë–ö–ò DTYPE:
    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ —á–∏—Å–ª–∞. –ï—Å–ª–∏ —Ç–∞–º —Ç–µ–∫—Å—Ç ‚Äî —Å—Ç–∞–Ω–µ—Ç NaN
    df['complexity'] = pd.to_numeric(df['complexity'], errors='coerce')

    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞—Å—å
    initial_count = len(df)
    df = df.dropna(subset=['complexity', 'text'])

    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç ‚Äî —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Ç—Ä–æ–∫–∏
    df['text'] = df['text'].astype(str)

    if len(df) < initial_count:
        print(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {initial_count - len(df)} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–º—É—Å–æ—Ä/–æ—à–∏–±–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è).")

    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º (–≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —É —Ç–µ–±—è –º—É—Å–æ—Ä –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞!)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)

    sentences = df['text'].tolist()
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤ float32 –¥–ª—è TensorFlow
    labels = df['complexity'].astype('float32').values

    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ß–∏—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df)}")
    return sentences, labels


def create_model():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (LSTM)."""
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(MAX_LEN,)),
        tf.keras.layers.Embedding(MAX_WORDS, 128),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
        tf.keras.layers.GlobalAveragePooling1D(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.4),  # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)  # –í—ã—Ö–æ–¥ ‚Äî –æ–¥–Ω–æ —á–∏—Å–ª–æ (—Å–ª–æ–∂–Ω–æ—Å—Ç—å)
    ])

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model


def main():
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(MODELS_DIR, exist_ok=True)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞
    try:
        sentences, labels = load_data(DATASET_PATH)
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
    tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True)
    tokenizer.fit_on_texts(sentences)
    sequences = tokenizer.texts_to_sequences(sentences)
    padded_data = pad_sequences(sequences, maxlen=MAX_LEN)

    # 3. –û–±—É—á–µ–Ω–∏–µ
    model = create_model()

    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ —É—á–∏—Ç—å—Å—è (–æ–±—ã—á–Ω–æ –Ω–∞ 20-35 —ç–ø–æ—Ö–µ –¥–ª—è 25–∫ —Å—Ç—Ä–æ–∫)
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    )

    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞—Ç–æ...")
    model.fit(
        padded_data,
        labels,
        epochs=100,
        batch_size=64,
        validation_split=0.15,
        callbacks=[early_stop],
        verbose=1
    )

    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–ø–∫—É models
    model_path = os.path.join(MODELS_DIR, 'complexity_model.keras')
    tokenizer_path = os.path.join(TOKENIZER_DIR, 'tokenizer.pickle')

    model.save(model_path)
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)

    print(f"\n‚ú® –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    print(f"üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    print(f"üì¶ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {tokenizer_path}")


if __name__ == "__main__":
    main()
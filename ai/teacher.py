import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô –ò –ü–ê–ü–û–ö ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'data_set')
MODELS_DIR = os.path.join(BASE_DIR, 'models')
TOKENIZER_DIR = os.path.join(BASE_DIR, 'tokenizers')
DATASET_PATH = os.path.join(DATA_DIR, 'data_set.csv')

if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

if not os.path.exists(DATASET_PATH):
    print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {DATASET_PATH}")
    exit()

# --- 2. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
# –ò—Å–ø–æ–ª—å–∑—É–µ–º sep=None –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (; –∏–ª–∏ ,) –∏ cp1251 –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
df = pd.read_csv(DATASET_PATH, sep=None, engine='python', encoding='utf-16')

df['text'] = df['text'].fillna(' ')
sentences = df['text'].astype(str).tolist()
complexity_labels = df['complexity'].fillna(0).values
social_labels = df['social'].fillna(0).values

# –ü–ê–†–ê–ú–ï–¢–†–´ (–í–∞–∂–Ω–æ!)
MAX_WORDS = 5000  # –°–∫–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –ø–æ–º–Ω–∏—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å
MAX_LEN = 20     # –î–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã (–≤ —Å–ª–æ–≤–∞—Ö)

# –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø (–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞ –≤ —á–∏—Å–ª–∞)
tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
padded_data = pad_sequences(sequences, maxlen=MAX_LEN)

# --- 3. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ù–ï–ô–†–û–°–ï–¢–ò ---
input_layer = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_layer')
x = tf.keras.layers.Embedding(MAX_WORDS, 64)(input_layer)
x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)

out_comp = tf.keras.layers.Dense(1, name='complexity_head')(x)
out_soc = tf.keras.layers.Dense(1, name='social_head')(x)

model = tf.keras.Model(inputs=input_layer, outputs=[out_comp, out_soc])

# --- 4. –ö–û–ú–ü–ò–õ–Ø–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï ---
model.compile(
    optimizer='adam',
    loss='mse',
    metrics={'complexity_head': 'mae', 'social_head': 'mae'}
)

print("\nüöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
model.fit(
    padded_data,
    {'complexity_head': complexity_labels, 'social_head': social_labels},
    epochs=100,
    batch_size=32,
    verbose=1
)

# --- 5. –§–£–ù–ö–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ---
def predict_action(text):
    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–∞–∫—É—é –∂–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–∏—Ñ—Ä, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    seq = tokenizer.texts_to_sequences([text])
    pad = pad_sequences(seq, maxlen=MAX_LEN)
    comp, soc = model.predict(pad, verbose=0)

    c = comp[0][0]
    s = soc[0][0]
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º XP: —Å–ª–æ–∂–Ω–æ—Å—Ç—å * —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –≤–µ—Å * 100
    total_xp = int(max(0, c * s * 100))

    print(f"\n--- –ê–Ω–∞–ª–∏–∑ ---")
    print(f"–¢–µ–∫—Å—Ç: {text} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {c:.2f} | –°–æ—Ü. –≤–µ—Å: {s:.2f} | XP: {total_xp}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞
predict_action("–ö–∞—Ç–Ω—É–ª –∫–∞—Ç–∫—É –≤ –¥–æ—Ç–∫—É")
predict_action("–ü—Ä–æ—á–∏—Ç–∞–ª —Å–ª–æ–∂–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º")

# --- 6. –°–û–•–†–ê–ù–ï–ù–ò–ï ---
model_path = os.path.join(MODELS_DIR, 'xp_model.keras')
tokenizer_path = os.path.join(TOKENIZER_DIR, 'tokenizer.pickle')

model.save(model_path)

with open(tokenizer_path, 'wb') as f:
    pickle.dump(tokenizer, f)

print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {MODELS_DIR}")
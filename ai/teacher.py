import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ---
MODELS_DIR = 'models'
TOKENIZERS_DIR = 'tokenizers'
DATASET_DIR = 'dataset'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(TOKENIZERS_DIR, exist_ok=True)

DATASET_PATH = os.path.join(DATASET_DIR, 'dataset.csv')

if not os.path.exists(DATASET_PATH):
    print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {DATASET_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    exit()

# --- 2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
try:
    try:
        df = pd.read_csv(DATASET_PATH, sep=';', encoding='utf-16')
    except UnicodeError:
        df = pd.read_csv(DATASET_PATH, sep=';', encoding='utf-8')

    df.columns = df.columns.str.strip()

    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—É—Å–æ—Ä –≤ –Ω–∞—á–∞–ª–µ!
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)

    sentences = df['text'].astype(str).tolist()
    labels = df['complexity'].values
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –í—Å–µ–≥–æ: {len(df)} —Å—Ç—Ä–æ–∫.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
    exit()

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ï–ö–°–¢–ê ---
MAX_WORDS = 10000  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è 17–∫ —Å—Ç—Ä–æ–∫
MAX_LEN = 30  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π

tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
padded_data = pad_sequences(sequences, maxlen=MAX_LEN)

# --- 4. –ú–û–î–ï–õ–¨ (–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–¥ 17–∫ —Å—Ç—Ä–æ–∫) ---
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(MAX_LEN,)),
    tf.keras.layers.Embedding(MAX_WORDS, 128),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    # –ü–æ–º–æ–≥–∞–µ—Ç –≤—ã–¥–µ–ª–∏—Ç—å —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# --- 5. –û–ë–£–ß–ï–ù–ò–ï ---
# –î–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—Ç–∞–≤–∏–º patience –ø–æ–±–æ–ª—å—à–µ
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 17 000 —Å—Ç—Ä–æ–∫ –Ω–∞—á–∞—Ç–æ...")
model.fit(
    padded_data,
    labels,
    epochs=100,
    batch_size=64,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    validation_split=0.15,  # 15% –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ç–∞–∫–æ–≥–æ –æ–±—ä–µ–º–∞
    callbacks=[early_stop],
    verbose=1
)

# --- 6. –°–û–•–†–ê–ù–ï–ù–ò–ï (–†–ê–ó–î–ï–õ–¨–ù–û–ï) ---
model_path = os.path.join(MODELS_DIR, 'complexity_model.keras')
tokenizer_path = os.path.join(TOKENIZERS_DIR, 'tokenizer.pickle')

model.save(model_path)
with open(tokenizer_path, 'wb') as f:
    pickle.dump(tokenizer, f)

print(f"\n‚ú® –ì–æ—Ç–æ–≤–æ!")
print(f"üì¶ –ú–æ–¥–µ–ª—å: {model_path}")
print(f"üì¶ –¢–æ–∫–µ–Ω: {tokenizer_path}")
import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ---
# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É models –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
MODELS_DIR = 'models'
TOKENIZERS_DIR = 'tokenizers'
DATASET_DIR = 'dataset'

#os.makedirs(MODELS_DIR, exist_ok=True)

# –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É (—Ç–µ–ø–µ—Ä—å –∏—â–µ–º –µ–≥–æ –≤ –∫–æ—Ä–Ω–µ –∏–ª–∏ —Ç–∞–º, –≥–¥–µ —Ç—ã –µ–≥–æ –ø–æ–ª–æ–∂–∏–ª)
DATASET_PATH = os.path.join(DATASET_DIR, 'dataset.csv')

if not os.path.exists(DATASET_PATH):
    print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {DATASET_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    exit()

# --- 2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
try:
    try:
        df = pd.read_csv(DATASET_PATH, sep=';', encoding='utf-16')
    except UnicodeError:
        df = pd.read_csv(DATASET_PATH, sep=';', encoding='utf-8')

    df.columns = df.columns.str.strip()
    sentences = df['text'].astype(str).tolist()
    labels = df['complexity'].values
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ó–∞–ø–∏—Å–µ–π: {len(df)}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
    exit()

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ï–ö–°–¢–ê ---
MAX_WORDS = 5000
MAX_LEN = 20

tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
padded_data = pad_sequences(sequences, maxlen=MAX_LEN)

# --- 4. –ú–û–î–ï–õ–¨ ---
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(MAX_LEN,)),
    tf.keras.layers.Embedding(MAX_WORDS, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# --- 5. –û–ë–£–ß–ï–ù–ò–ï ---
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞—Ç–æ...")
model.fit(
    padded_data,
    labels,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# --- 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –í .KERAS ---
# –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É models
model_path = os.path.join(MODELS_DIR, 'complexity_model.keras')
tokenizer_path = os.path.join(TOKENIZERS_DIR, 'tokenizer.pickle')

model.save(model_path)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ

with open(tokenizer_path, 'wb') as f:
    pickle.dump(tokenizer, f)

print(f"\n‚ú® –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É '{MODELS_DIR}'")